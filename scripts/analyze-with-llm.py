#!/usr/bin/env python3
"""
LLM Deep Analysis Script - Phase 2
Analysiert Blog-Posts mit LLM f√ºr tiefgehendes Verst√§ndnis

Verwendung:
    GROQ_API_KEY=xxx python scripts/analyze-with-llm.py

    Optional mit OpenAI:
    OPENAI_API_KEY=xxx python scripts/analyze-with-llm.py --provider openai

Output:
    data/blog-intelligence.json
"""

import os
import sys
import json
import time
import argparse
from pathlib import Path
from datetime import datetime

# Konfiguration
PROJECT_ROOT = Path(__file__).parent.parent
INPUT_FILE = PROJECT_ROOT / "data" / "blog-content-raw.json"
OUTPUT_FILE = PROJECT_ROOT / "data" / "blog-intelligence.json"

# Rate Limiting (Groq: 30 req/min, OpenAI: 60 req/min)
RATE_LIMIT_DELAY = 2.5  # Sekunden zwischen Requests


def get_analysis_prompt(article):
    """Generiert den Analyse-Prompt f√ºr einen Artikel"""
    return f"""Du bist ein einf√ºhlsamer Content-Analyst f√ºr eine Coaching-Website.
Die Website geh√∂rt Kathrin Stahl, einer Coachin die Menschen durch Lebenskrisen, Selbstfindung und pers√∂nliches Wachstum begleitet. Sie arbeitet viel mit Pferden, Meditation und systemischen Aufstellungen.

Analysiere diesen Blog-Post TIEFGEHEND. Erfasse nicht nur die Oberfl√§che, sondern die emotionale und transformative Tiefe.

---
TITEL: {article['title']}

KATEGORIE: {article.get('category', 'unbekannt')}

KERNAUSSAGEN (Blockquotes):
{chr(10).join(['- ' + bq for bq in article.get('blockquotes', [])][:5]) or '(keine)'}

INHALT:
{article['content'][:4000]}
---

Antworte NUR mit einem validen JSON-Objekt (keine Erkl√§rungen, kein Markdown):

{{
  "kernbotschaft": "Die zentrale Erkenntnis in 1-2 S√§tzen. Was soll der Leser wirklich verstehen?",

  "emotionaleTonalitaet": "EINE von: troestend | aktivierend | reflektierend | heilend | ermutigend | konfrontierend | liebevoll | transformierend",

  "transformation": {{
    "von": "Ausgangszustand des Lesers (z.B. 'Selbstzweifel', 'innerer Kampf')",
    "zu": "Zielzustand (z.B. 'Selbstakzeptanz', 'innere Ruhe')"
  }},

  "tiefenthemen": [
    "Spezifische Themen, NICHT generisch!",
    "NICHT: 'angst', 'selbstliebe'",
    "SONDERN: 'angst-vor-eigener-groesse', 'koerper-als-verbuendeter', 'loslassen-durch-annehmen'"
  ],

  "lebensphase": ["Passende aus: midlife | neuanfang | beziehungskrise | burnout | selbstfindung | trauerarbeit | berufliche-neuorientierung | alltag"],

  "coachingMethode": ["Falls erkennbar: pferde | meditation | aufstellung | journaling | koerperarbeit | visualisierung | achtsamkeit"],

  "leserProfil": "Kurze Beschreibung wer diesen Artikel lesen sollte (1 Satz)",

  "empfehlungsBegr√ºndungen": [
    "Pers√∂nliche Begr√ºndung 1: Warum sollte jemand diesen Artikel nach einem √§hnlichen lesen? Direkt, 2. Person.",
    "Pers√∂nliche Begr√ºndung 2: Anderer Aspekt",
    "Pers√∂nliche Begr√ºndung 3: Noch ein Aspekt"
  ]
}}"""


def analyze_with_groq(article, api_key):
    """Analysiert einen Artikel mit Groq (Llama 3)"""
    from groq import Groq

    client = Groq(api_key=api_key)
    prompt = get_analysis_prompt(article)

    try:
        response = client.chat.completions.create(
            model="llama-3.3-70b-versatile",
            messages=[
                {
                    "role": "system",
                    "content": "Du bist ein pr√§ziser JSON-Generator. Antworte NUR mit validem JSON, ohne Markdown-Formatierung, ohne Erkl√§rungen."
                },
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
            max_tokens=1500
        )

        result_text = response.choices[0].message.content.strip()

        # Bereinige m√∂gliche Markdown-Wrapper
        if result_text.startswith('```'):
            result_text = result_text.split('```')[1]
            if result_text.startswith('json'):
                result_text = result_text[4:]
        result_text = result_text.strip()

        return json.loads(result_text)

    except json.JSONDecodeError as e:
        print(f"    ‚ö†Ô∏è  JSON Parse Error: {e}")
        print(f"    Response: {result_text[:200]}...")
        return None
    except Exception as e:
        print(f"    ‚ùå API Error: {e}")
        return None


def analyze_with_openai(article, api_key):
    """Analysiert einen Artikel mit OpenAI GPT-4"""
    from openai import OpenAI

    client = OpenAI(api_key=api_key)
    prompt = get_analysis_prompt(article)

    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": "Du bist ein pr√§ziser JSON-Generator. Antworte NUR mit validem JSON."
                },
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
            max_tokens=1500,
            response_format={"type": "json_object"}
        )

        result_text = response.choices[0].message.content.strip()
        return json.loads(result_text)

    except Exception as e:
        print(f"    ‚ùå API Error: {e}")
        return None


def is_real_llm_analysis(analysis):
    """Pr√ºft ob eine Analyse von einem LLM stammt oder ein Fallback ist"""
    if not analysis:
        return False

    # Fallback-Analysen haben diese generischen Werte
    fallback_indicators = [
        analysis.get('transformation', {}).get('von') == 'Suche',
        analysis.get('transformation', {}).get('zu') == 'Erkenntnis',
        analysis.get('leserProfil') == 'Menschen auf der Suche nach Orientierung',
        'Dieser Artikel k√∂nnte dir neue Perspektiven er√∂ffnen.' in analysis.get('empfehlungsBegr√ºndungen', [])
    ]

    # Wenn 3+ Indikatoren zutreffen, ist es wahrscheinlich ein Fallback
    return sum(fallback_indicators) < 3


def create_default_analysis(article):
    """Erstellt eine Fallback-Analyse wenn LLM fehlschl√§gt

    WICHTIG: Diese Fallback-Analysen werden mit _isFallback=True markiert,
    damit sie sp√§ter von echten LLM-Analysen √ºberschrieben werden k√∂nnen.
    """
    category_themes = {
        'achtsamkeit': ['praesenz', 'innere-ruhe', 'moment'],
        'selbstliebe': ['selbstakzeptanz', 'selbstwert', 'innerer-kritiker'],
        'beziehung': ['kommunikation', 'verbindung', 'grenzen'],
        'heldinnenreise': ['lebenssinn', 'transformation', 'berufung'],
        'hochbegabung': ['anderssein', 'hochsensibilitaet', 'potential'],
        'koerper': ['koerperbewusstsein', 'symptome', 'heilung']
    }

    category = article.get('category', 'allgemein')

    return {
        "_isFallback": True,  # Marker f√ºr Fallback-Analyse
        "kernbotschaft": article.get('excerpt', '')[:150],
        "emotionaleTonalitaet": "reflektierend",
        "transformation": {
            "von": "Suche",
            "zu": "Erkenntnis"
        },
        "tiefenthemen": category_themes.get(category, ['selbsterkenntnis', 'wachstum']),
        "lebensphase": ["selbstfindung"],
        "coachingMethode": [],
        "leserProfil": "Menschen auf der Suche nach Orientierung",
        "empfehlungsBegr√ºndungen": [
            "Dieser Artikel k√∂nnte dir neue Perspektiven er√∂ffnen.",
            "Hier findest du weitere Impulse zu diesem Thema.",
            "Ein anderer Blickwinkel auf das, was dich besch√§ftigt."
        ]
    }


def main():
    parser = argparse.ArgumentParser(description='LLM Deep Analysis f√ºr Blog-Posts')
    parser.add_argument('--provider', choices=['groq', 'openai'], default='groq',
                        help='LLM Provider (default: groq)')
    parser.add_argument('--limit', type=int, default=None,
                        help='Nur N Artikel analysieren (f√ºr Tests)')
    parser.add_argument('--skip-existing', action='store_true',
                        help='√úberspringe bereits analysierte Artikel')
    args = parser.parse_args()

    print("=" * 60)
    print("LLM DEEP ANALYSIS - Phase 2")
    print(f"Provider: {args.provider.upper()}")
    print("=" * 60)
    print()

    # API Key pr√ºfen
    if args.provider == 'groq':
        api_key = os.environ.get('GROQ_API_KEY')
        if not api_key:
            print("‚ùå GROQ_API_KEY nicht gesetzt!")
            print("   Setze: export GROQ_API_KEY=dein_key")
            print("   Oder hol dir einen kostenlosen Key: https://console.groq.com")
            sys.exit(1)
        analyze_fn = lambda a: analyze_with_groq(a, api_key)
    else:
        api_key = os.environ.get('OPENAI_API_KEY')
        if not api_key:
            print("‚ùå OPENAI_API_KEY nicht gesetzt!")
            sys.exit(1)
        analyze_fn = lambda a: analyze_with_openai(a, api_key)

    # Lade extrahierte Inhalte
    if not INPUT_FILE.exists():
        print(f"‚ùå Input-Datei nicht gefunden: {INPUT_FILE}")
        print("   F√ºhre zuerst aus: python scripts/extract-blog-content.py")
        sys.exit(1)

    with open(INPUT_FILE, 'r', encoding='utf-8') as f:
        data = json.load(f)

    articles = data['articles']

    # Lade bestehende Analysen (falls vorhanden)
    # WICHTIG: Echte LLM-Analysen werden IMMER gesch√ºtzt, Fallbacks k√∂nnen √ºberschrieben werden
    existing_analyses = {}
    protected_analyses = {}  # Echte LLM-Analysen die nicht √ºberschrieben werden d√ºrfen
    fallback_analyses = {}   # Fallback-Analysen die √ºberschrieben werden k√∂nnen

    if OUTPUT_FILE.exists():
        with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:
            existing_data = json.load(f)
            for art in existing_data.get('articles', []):
                if art.get('analysis'):
                    url = art['url']
                    analysis = art['analysis']

                    # Pr√ºfe ob es eine echte LLM-Analyse ist
                    if analysis.get('_isFallback') or not is_real_llm_analysis(analysis):
                        fallback_analyses[url] = analysis
                    else:
                        protected_analyses[url] = analysis

        print(f"üìö {len(protected_analyses)} echte LLM-Analysen (gesch√ºtzt)")
        print(f"üìù {len(fallback_analyses)} Fallback-Analysen (k√∂nnen √ºberschrieben werden)")

    # Bei --skip-existing: √úberspringe ALLE bestehenden Analysen
    # Ohne Flag: √úberspringe nur gesch√ºtzte (echte) Analysen, versuche Fallbacks zu ersetzen
    if args.skip_existing:
        existing_analyses = {**protected_analyses, **fallback_analyses}
    else:
        existing_analyses = protected_analyses  # Nur echte Analysen sch√ºtzen

    # Filtere nur Blog-Posts (keine Quizze, Angebote etc.)
    blog_articles = [a for a in articles if a.get('type') == 'blog']

    if args.limit:
        blog_articles = blog_articles[:args.limit]

    print(f"üìä {len(blog_articles)} Blog-Posts zu analysieren")
    print()

    # Analysiere jeden Artikel
    results = []
    success_count = 0
    error_count = 0

    for i, article in enumerate(blog_articles):
        url = article['url']
        title = article['title'][:40]

        print(f"[{i+1}/{len(blog_articles)}] {title}...")

        # √úberspringe bereits analysierte (echte LLM-Analysen sind immer gesch√ºtzt)
        if url in existing_analyses:
            article['analysis'] = existing_analyses[url]
            results.append(article)
            if url in protected_analyses:
                print(f"    üõ°Ô∏è  Gesch√ºtzt (echte LLM-Analyse)")
            else:
                print(f"    ‚è≠Ô∏è  √úbersprungen (bereits analysiert)")
            continue

        # LLM Analyse
        analysis = analyze_fn(article)

        if analysis:
            article['analysis'] = analysis
            success_count += 1
            print(f"    ‚úì Analysiert: {analysis.get('emotionaleTonalitaet', '?')}")
        else:
            article['analysis'] = create_default_analysis(article)
            error_count += 1
            print(f"    ‚ö†Ô∏è  Fallback verwendet")

        results.append(article)

        # Rate Limiting
        if i < len(blog_articles) - 1:
            time.sleep(RATE_LIMIT_DELAY)

    print()
    print("-" * 60)
    print("STATISTIKEN")
    print("-" * 60)
    print(f"  Erfolgreich:    {success_count}")
    print(f"  Fallback:       {error_count}")
    print(f"  Gesamt:         {len(results)}")
    print()

    # Speichere Ergebnis
    output_data = {
        'analyzedAt': datetime.now().isoformat(),
        'provider': args.provider,
        'totalArticles': len(results),
        'articles': results
    }

    OUTPUT_FILE.parent.mkdir(parents=True, exist_ok=True)
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)

    print(f"üíæ Gespeichert: {OUTPUT_FILE}")
    print()
    print("‚úÖ Phase 2 abgeschlossen!")
    print("   N√§chster Schritt: python scripts/generate-embeddings.py")


if __name__ == '__main__':
    main()
